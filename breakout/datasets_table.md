| Name                                                                             | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Data Type Visualized                                                               | Task                                                                                                            | Eye Tracking System | Eye Tracker Model                                 | FPS                                                                                                  | Dataset size                                                                                                                                                                              | Data structure                                                                                                                                                                                                         | Domain(s)                                     | Application                                                                                                                  | Modalities                                                                                               | URL                                                                                                                                                                                                                                                                                                                                                  | License                                                       | Year | Reference                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Number of Citations | Paper                                                                                                                                                                                                                                                                                    |
| -------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------- | ------------------- | ------------------------------------------------- | ---------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------- | ---- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Potsdam Sentence Corpus                                                          | Collection of 144 German sentences with eye-tracking from 222 participants reading these sentences.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | Sequence of text paragraphs displayed on screen.                                   | Understand reading process                                                                                      | Remote/screen based | EyeLink                                           | 250/500                                                                                              | 222 readers                                                                                                                                                                               | [https://publishup.uni-potsdam.de/opus4-ubp/frontdoor/deliver/index/docId/5521/file/phr_263_2011_12_13.pdf](https://publishup.uni-potsdam.de/opus4-ubp/frontdoor/deliver/index/docId/5521/file/phr_263_2011_12_13.pdf) | Language understanding                        | Measure influence of Past, Present, and Future Words on Fixation Durations                                                   | text, fixations                                                                                          | [https://publishup.uni-potsdam.de/opus4-ubp/frontdoor/deliver/index/docId/5521/file/phr_263_2011_12_13.pdf](https://publishup.uni-potsdam.de/opus4-ubp/frontdoor/deliver/index/docId/5521/file/phr_263_2011_12_13.pdf)                                                                                                                               | N/A                                                           | 2003 | [Kliegl, R., Nuthmann, A., & Engbert, R. (2006). Tracking the mind during reading: The influence of past, present, and future words on fixation durations. Journal of Experimental Psychology: General, 135(1), 12–35. https://doi.org/10.1037/0096-3445.135.1.12](https://psycnet.apa.org/doi/10.1037/0096-3445.135.1.12)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 777                 | [https://doi.org/10.1037/0096-3445.135.1.12](https://doi.org/10.1037/0096-3445.135.1.12)                                                                                                                                                                                                 |
| SearchModels                                                                     | Research relation between emotional properties of an image and visual attention.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | Natural images                                                                     | Model visual attention                                                                                          | Remote/screen based | ISCAN RK-464                                      | 240                                                                                                  | 912 outdoor scenes size: 800x600px<br>1 dva ~ 34px, 14 observers                                                                                                                          | N/A                                                                                                                                                                                                                    | Visual attention                              | Understand visual attention in real world settings                                                                           | images, fixations                                                                                        | N/A                                                                                                                                                                                                                                                                                                                                                  | N/A                                                           | 2009 | Ehinger, Krista A., et al. "Modelling search for people in 900 scenes: A combined source model of eye guidance." Visual cognition 17.6-7 (2009): 945-978.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 420                 | [http://olivalab.mit.edu/Papers/EhingerHidalgoTorralbaOliva_VisCog2009.pdf](http://olivalab.mit.edu/Papers/EhingerHidalgoTorralbaOliva_VisCog2009.pdf)                                                                                                                                   |
| MIT300                                                                           | This was the first data set with held-out human eye movements, and is used as benchmark test set in the MIT/Tübingen Saliency Benchmark.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | Natural images                                                                     | Model visual attention                                                                                          | Mobile eye-tracker  | ETL 400 ISCAN                                     | 240                                                                                                  | 300 natural indoor and outdoor scenes, size: max dim: 1024px, other dim: 457-1024px, 39 observers<br>ages: 18-50                                                                          | [https://saliency.tuebingen.ai/datasets.html](https://saliency.tuebingen.ai/datasets.html)                                                                                                                             | First person visual attention                 | A benchmark for saliency predictions of computational models for visual attention                                            | images, fixations                                                                                        | [https://saliency.tuebingen.ai/results.html](https://saliency.tuebingen.ai/results.html)                                                                                                                                                                                                                                                             | N/A                                                           | 2012 | Judd, Tilke, Frédo Durand, and Antonio Torralba. "A benchmark of computational models of saliency to predict human fixations." (2012).<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 652                 | [https://dspace.mit.edu/handle/1721.1/68590](https://dspace.mit.edu/handle/1721.1/68590)                                                                                                                                                                                                 |
| Dundee Corpus: An eye movement analysis                                          | a large corpus of eye movements from ten native English speakers (and ten native French speakers) reading texts from newspaper editorials (56,212 tokens). Texts were presented on-screen in a multiline format. For a subset of the texts (16 four-line paragraphs), predictability data were obtained for each word (272 participants total, making approximately 25 responses per word).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Sequence of text paragraphs displayed on screen.                                   | Understand reading process                                                                                      | N/A                 | N/A                                               | N/A                                                                                                  | N/A                                                                                                                                                                                       | [https://journals.sagepub.com/doi/10.1080/17470218.2012.676054](https://journals.sagepub.com/doi/10.1080/17470218.2012.676054)                                                                                         | Language understanding                        | Evaluate whether word frequency and word predictability have early interactive effects on inspection time                    | text, fixations                                                                                          | N/A                                                                                                                                                                                                                                                                                                                                                  | N/A                                                           | 2013 | Kennedy A, Pynte J, Murray WS, Paul SA. Frequency and predictability effects in the Dundee Corpus: an eye movement analysis. Q J Exp Psychol (Hove). 2013;66(3):601-18. doi: 10.1080/17470218.2012.676054. Epub 2012 May 29. PMID: 22643118.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 97                  | [https://pubmed.ncbi.nlm.nih.gov/22643118/](https://pubmed.ncbi.nlm.nih.gov/22643118/)                                                                                                                                                                                                   |
| EyeC3D                                                                           | Eight stereoscopic video sequences were used in the eye tracking experiments. For each video, eye movement data was recorded via a set of subjective experiments. From the eye movement data, the fixation density maps (FDMs) were computed for each frame of the stereoscopic video sequences; 21 participants (16M/5F)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | 8 stereoscopic videos visualized for 8-10s on a screen                             | Model visual attention                                                                                          | Remote/screen based | Smart Eye Pro 5.8                                 | 60 Hz                                                                                                | 21 subjects with a total of ?? fixation points                                                                                                                                            | [https://www.epfl.ch/labs/mmspg/downloads/eyec3d/](https://www.epfl.ch/labs/mmspg/downloads/eyec3d/)                                                                                                                   | Visual attention, CV                          | Stereoscopic Saliency prediction                                                                                             | videos, fixations                                                                                        | [https://www.epfl.ch/labs/mmspg/downloads/eyec3d/](https://www.epfl.ch/labs/mmspg/downloads/eyec3d/)                                                                                                                                                                                                                                                 | Research purpose only                                         | 2014 | Philippe H, Touradj E..<br>In 2014 Sixth International Workshop on Quality of Multimedia Experience (QoMEX)EyeC3D: 3D video eye tracking dataset. 2014. (pp. 55-56)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | 15                  | [https://ieeexplore.ieee.org/document/6982290](https://ieeexplore.ieee.org/document/6982290)                                                                                                                                                                                             |
| Gazefollow                                                                       | A large-scale dataset annotated with the location of where people in images are looking. The dataset is collated from several major datasets that contain people as a source of images:SUN,MSCOCO, Actions40, PASCAL, ImageNet detection challenge and Places dataset.This concatenation results in a challenging and large image collection of people performing diverse activities in many everyday scenarios. Since the source datasets do not have gaze ground-truth,we annotated it using Amazon’s Mechanical Turk(AMT).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Natural images                                                                     | Model visual attention                                                                                          | N/A                 | N/A                                               | N/A                                                                                                  | 130,339 people in 122,143 images,with gaze locations inside the image. 4,782 people of the dataset was used for testing and the rest for training                                         | [http://gazefollow.csail.mit.edu/download.html](http://gazefollow.csail.mit.edu/download.html)                                                                                                                         | Thrid person perspective for interactive gaze | Understanding human interactions with scene objects from a thrid person perspective                                          | images, fixations                                                                                        | [http://gazefollow.csail.mit.edu/index.html](http://gazefollow.csail.mit.edu/index.html)                                                                                                                                                                                                                                                             | Research purpose only                                         | 2015 | Where are they looking? A. Recasens, A. Khosla, C. Vondrick and A. Torralba, Advances in Neural Information Processing Systems (NIPS), 2015                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | 236                 | [https://people.csail.mit.edu/khosla/papers/nips2015_recasens.pdf](https://people.csail.mit.edu/khosla/papers/nips2015_recasens.pdf)                                                                                                                                                     |
| CAT                                                                              | This dataset contains two sets of images: train and test. Train images (100 from each category) and fixations of 18 observers are shared but 6 observers are held-out. Test images are available but fixations of all 24 observers are held out.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | Natural images                                                                     | Model visual attention                                                                                          | Remote/screen based | EyeLink 1000                                      | 1000                                                                                                 | 4000 images from 20 different categories, size: 1920x1080px, 24 observers per image                                                                                                       | [https://saliency.tuebingen.ai/datasets.html](https://saliency.tuebingen.ai/datasets.html)                                                                                                                             | First person visual attention                 | Advances saliency modeling and helps conduct behavioral studies on bottom-up visual attention                                | images, fixations                                                                                        | [https://saliency.tuebingen.ai/datasets.html](https://saliency.tuebingen.ai/datasets.html)                                                                                                                                                                                                                                                           | N/A                                                           | 2015 | Borji, Ali, and Laurent Itti. "Cat2000: A large scale fixation dataset for boosting saliency research." arXiv preprint arXiv:1505.03581 (2015).<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | 342                 | [https://arxiv.org/abs/1505.03581](https://arxiv.org/abs/1505.03581)                                                                                                                                                                                                                     |
| GazeCapture                                                                      | 450 people consisting of almost 2.5M frames. Using GazeCapture, we train iTracker, a convolutional neural network for eye tracking, which achieves a significant reduction in error over previous approaches while running in real time (10 - 15fps) on a modern mobile device. Our model achieves a prediction error of 1.7cm and 2.5cm without calibration on mobile phones and tablets respectively. With calibration, this is reduced to 1.3cm and 2.1cm. Further, we demonstrate that the features learned by iTracker generalize well to other datasets, achieving state-of-the-art results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | Random distribution of gaze points on screen synthetically generated               | Model visual attention                                                                                          | Remote/screen based | iPhone or iPad cameras                            | 10-15 fps                                                                                            | 1474 subjects' selfies, total 2.5 million frames                                                                                                                                          | [https://github.com/CSAILVision/GazeCapture](https://github.com/CSAILVision/GazeCapture)                                                                                                                               | CV                                            | 2D Gaze estimation. I.e. infer gaze location on 2D screen.                                                                   | images, fixations                                                                                        | [https://gazecapture.csail.mit.edu/<br><br>https://github.com/CSAILVision/GazeCapture](https://gazecapture.csail.mit.edu/https:/github.com/CSAILVision/GazeCapture)                                                                                                                                                                                  | Research License                                              | 2016 | Krafka K, Khosla A, Kellnhofer P, Kannan H, Bhandarkar S, Matusik W, Torralba A. Eye tracking for everyone. InProceedings of the IEEE conference on computer vision and pattern recognition 2016 (pp. 2176-2184).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 955                 | [https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Krafka_Eye_Tracking_for_CVPR_2016_paper.pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Krafka_Eye_Tracking_for_CVPR_2016_paper.pdf)                                                           |
| GECO: An eyetracking corpus of monolingual and bilingual sentence reading<br>    | A monolingual and bilingual corpus of the eyetracking data of participants reading a complete novel.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Sequence of text paragraphs displayed on screen.                                   | Understand reading process                                                                                      | Remote/screen based | EyeLink 1000                                      | 1000 fps                                                                                             | 19 bilingual subjects and 14 monolingual subjects                                                                                                                                         | [https://link.springer.com/article/10.3758/s13428-016-0734-0#Sec10](https://link.springer.com/article/10.3758/s13428-016-0734-0#Sec10)                                                                                 | Language understanding                        | Understand differences in language knowledge between monolingual and bilingual participants                                  | text, fixations                                                                                          | [https://expsy.ugent.be/downloads/geco/](https://expsy.ugent.be/downloads/geco/)                                                                                                                                                                                                                                                                     | N/A                                                           | 2016 | Cop, Uschi, et al. "Presenting GECO: An eyetracking corpus of monolingual and bilingual sentence reading." Behavior research methods 49 (2017): 602-615.<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 212                 | [https://link.springer.com/article/10.3758/s13428-016-0734-0](https://link.springer.com/article/10.3758/s13428-016-0734-0)                                                                                                                                                               |
| Dr(eye)ve                                                                        | Composed by 74 video sequences of 5 mins each, we have captured and annotated more than 500,000 frames. The labeling contains driversâ€™ gaze fixations and their temporal integration providing task-specific saliency maps. Geo-referenced locations, driving speed and course complete the set of released data. Dataset aims to give better understanding, exploiting and reproducing the driver’s attention process in the autonomous and assisted cars                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Natural vision through glasses                                                     | Exploit and reproduce the driver’s attention process in the autonomous and assisted cars of future generations. | Near/glasses        | Commercial SMI ETG 2w eye tracking glasses (ETG). | 60 fps gaze, 30 fps driver perspective from glasses, 25 fps car perspective from roof mounted camera | 75 videos by 8 subjects                                                                                                                                                                   | [https://github.com/ndrplz/dreyeve](https://github.com/ndrplz/dreyeve)                                                                                                                                                 | Autonomous driving                            | Studying application based human attention                                                                                   | 2D Screen Fixations, 2D Saliency, EEG                                                                    | [https://aimagelab.ing.unimore.it/imagelab/page.asp?IdPage=8](https://aimagelab.ing.unimore.it/imagelab/page.asp?IdPage=8)                                                                                                                                                                                                                           | Creative Commons Attribution 4.0 International Public License | 2016 | Alletto S, Palazzi A, Solera F, Calderara S, Cucchiara R. Dr (eye) ve: a dataset for attention-based tasks with applications to autonomous and assisted driving. InProceedings of the ieee conference on computer vision and pattern recognition workshops 2016 (pp. 54-60).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 116                 | [https://www.cv-foundation.org//openaccess/content_cvpr_2016_workshops/w3/papers/Alletto_DREyeVe_A_Dataset_CVPR_2016_paper.pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2016_workshops/w3/papers/Alletto_DREyeVe_A_Dataset_CVPR_2016_paper.pdf)                            |
| The Provo Corpus: A large eye tracking corpus with predictability norms          | Dataset consists of two parts, predictability norms and eye-tracking data. The predictability norms consist of completion norms for every word in 55 paragraphs. The eye-tracking corpus consists of eye movement data from 84 native English-speaking participants, who read all 55 paragraphs for comprehension.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | Sequence of text paragraphs displayed on screen.                                   | Understand reading process                                                                                      | Remote/screen based | SR Research EyeLink 1000 Plus                     | 1000 fps                                                                                             | 84 subjects reading 55 paragraphs                                                                                                                                                         | [https://link.springer.com/article/10.3758/s13428-017-0908-4/tables/2](https://link.springer.com/article/10.3758/s13428-017-0908-4/tables/2)                                                                           | Cognitive psychology, Language Understanding  | Ideal for studying predictive processes in reading                                                                           | text, fixations                                                                                          | [https://osf.io/sjefs/](https://osf.io/sjefs/)                                                                                                                                                                                                                                                                                                       | Creative Commons Attribution 4.0 International Public License | 2018 | Luke SG, Christianson K. The Provo Corpus: A large eye-tracking corpus with predictability norms. Behavior research methods. 2018 Apr;50:826-33.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | 117                 | [https://link.springer.com/article/10.3758/s13428-017-0908-4](https://link.springer.com/article/10.3758/s13428-017-0908-4)                                                                                                                                                               |
| Extended GTEA Gaze+                                                              | Joint gaze estimation and action recognition in First Person Vision.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Natural vision through glasses                                                     | Action recognition                                                                                              | Mobile eye-tracker  | SMI wearable eye-tracker                          | 60/120                                                                                               | 28 hours (de-identified) of cooking activities from 86 unique sessions of 32 subjects                                                                                                     | [https://cbs.ic.gatech.edu/fpv/](https://cbs.ic.gatech.edu/fpv/)                                                                                                                                                       | First person visual attention                 | First Person Vision (FPV), with an emphasis on FPV action recognition and gaze estimation                                    | videos, audios, fixations, frame-level action annotations, and pixel-level hand masks at sampled frames. | [https://cbs.ic.gatech.edu/fpv/](https://cbs.ic.gatech.edu/fpv/)                                                                                                                                                                                                                                                                                     | N/A                                                           | 2018 | Li, Yin, Miao Liu, and James M. Rehg. "In the eye of beholder: Joint learning of gaze and actions in first person video." Proceedings of the European conference on computer vision (ECCV). 2018.<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | 300                 | [https://openaccess.thecvf.com/content_ECCV_2018/papers/Yin_Li_In_the_Eye_ECCV_2018_paper.pdf](https://openaccess.thecvf.com/content_ECCV_2018/papers/Yin_Li_In_the_Eye_ECCV_2018_paper.pdf)                                                                                             |
| EMOtional                                                                        | [The images cover a diversity of emotional contents, arousing various sentiments, such as happiness, excitement, awe, disgust, and fear. Object-level and image-level annotations, code, and CNN models for saliency prediction are available on the project page.](http://ncript.comp.nus.edu.sg/site/ncript-top/emotionalattention/)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Natural images                                                                     | Model visual attention                                                                                          | Remote/screen based | Eyelink 1000                                      | 1000                                                                                                 | 1019 emotion-eliciting images, including 321 emotion-evoking pictures selected from IAPS, size: 1024x768px, 16 observers ages: 21-35                                                      | [https://ncript.comp.nus.edu.sg/site/ncript-top/emotionalattention/](https://ncript.comp.nus.edu.sg/site/ncript-top/emotionalattention/)                                                                               | Visual attention                              | Understand how image sentiment influences visual perception                                                                  | images, fixations                                                                                        | [https://ncript.comp.nus.edu.sg/site/ncript-top/emotionalattention/](https://ncript.comp.nus.edu.sg/site/ncript-top/emotionalattention/)                                                                                                                                                                                                             | N/A                                                           | 2018 | Fan, Shaojing, et al. "Emotional attention: A study of image sentiment and visual attention." Proceedings of the IEEE Conference on computer vision and pattern recognition. 2018.<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 160                 | [https://ncript.comp.nus.edu.sg/site/nus-files/emotionattention/CVPR2018/2361.pdf](https://ncript.comp.nus.edu.sg/site/nus-files/emotionattention/CVPR2018/2361.pdf)                                                                                                                     |
| MOET                                                                             | A publicly available Multiple Object Eye-Tracking (MOET) dataset, consisting of gaze data from participants tracking specific objects, annotated with labels and bounding boxes, in crowded real-world videos, for training and evaluating attention decoding algorithms                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | Green ellipse object tracking                                                      | Model visual attention                                                                                          | Mobile eye-tracker  | SMI RED-250                                       | 60                                                                                                   | 16 participants viewing the 14 videos of Multiple Object Tracking 2016 (MOT16) benchmark dataset  (11 221 frames totaling 7 min, 43 s in length, mean 33.0 s per video, std. dev. 18.9 s) | [https://osf.io/28rnx/](https://osf.io/28rnx/)                                                                                                                                                                         | Visual attention                              | Participants tracked target object, indicated by a green ellipse, as continuously as possible, with their eyes.              | videos, bounding boxes, gaze positions                                                                   | [https://github.com/karan-uppal3/decoding-attention](https://github.com/karan-uppal3/decoding-attention)                                                                                                                                                                                                                                             | Creative Commons Attribution 4.0 International Public License | 2020 | Uppal, Karan, Jaeah Kim, and Shashank Singh. "Decoding Attention from Gaze: A Benchmark Dataset and End-to-End Models." Annual Conference on Neural Information Processing Systems. PMLR, 2023.<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | 3                   | [https://arxiv.org/abs/2211.10966](https://arxiv.org/abs/2211.10966)                                                                                                                                                                                                                     |
| EVE                                                                              | To observe that there exists a strong relationship between what users are looking at and the appearance of the user’s eyes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Multimodal (videos: images, text)                                                  | Model visual attention                                                                                          | Remote/screen based | Tobii Pro Spectrum                                | 60                                                                                                   | 54 participants and consists of 4 camera views, over 12 million frames and 1327 unique visual stimuli (images, video, text), adding up to approximately 105 hours of video data in tota   | [https://ait.ethz.ch/eve](https://ait.ethz.ch/eve)                                                                                                                                                                     | Visual attention                              | Release a dataset and accompanying method which aims to explicitly learn semantic and temporal relationships                 | images, videos, text, fixations                                                                          | [https://ait.ethz.ch/eve](https://ait.ethz.ch/eve)                                                                                                                                                                                                                                                                                                   | CC BY-NC-SA 4.0 DEED                                          | 2020 | Park, Seonwook, et al. "Towards end-to-end video-based eye-tracking." Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XII 16. Springer International Publishing, 2020.<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 59                  | [https://arxiv.org/pdf/2007.13120.pdf](https://arxiv.org/pdf/2007.13120.pdf)                                                                                                                                                                                                             |
| Atari-HEAD                                                                       | A large-scale, high-quality dataset of human actions with simultaneously recorded eye movements while humans play Atari video games                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | Videos                                                                             | Model visual attention                                                                                          | Remote/screen based | EyeLink 1000                                      | 1000                                                                                                 | 4 subjects with 117.07 hours                                                                                                                                                              | [https://zenodo.org/records/2603190](https://zenodo.org/records/2603190)                                                                                                                                               | Visual attention                              | Decision making prediction                                                                                                   | videos, fixations                                                                                        | [https://zenodo.org/records/2587121](https://zenodo.org/records/2587121)                                                                                                                                                                                                                                                                             | Creative Commons Attribution 4.0 International Public License | 2020 | Zhang, Ruohan, et al. "Atari-head: Atari human eye-tracking and demonstration dataset." Proceedings of the AAAI conference on artificial intelligence. Vol. 34. No. 04. 2020.<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 65                  | [https://arxiv.org/pdf/1903.06754.pdf](https://arxiv.org/pdf/1903.06754.pdf)                                                                                                                                                                                                             |
| DGaze                                                                            | It contains data recorded of users interacting in dynamic virtual scenes using a virtual reality headset.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | 3D scenes                                                                          | Model visual attention                                                                                          | Mobile eye-tracker  | HTC Vive                                          | 100                                                                                                  | 43 participants watching 3D scenes , 18,000 gaze positions, 18,000 object positions, 36,000 head velocities, and 10,800 frames of scene screenshots.                                      | [https://zhiminghu.net/hu20_dgaze.html](https://zhiminghu.net/hu20_dgaze.html)                                                                                                                                         | Visual attention                              | Dynamic sceen prediction of current and future gaze of VR headset wearer, with closest application in gaming.                | videos, fixations , head velocities, and the positions of the nearest 3 objects (3\*3)                   | [https://chinapku-my.sharepoint.com/personal/1701111311_pku_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments%2FDGazeDataset&ga=1](https://chinapku-my.sharepoint.com/personal/1701111311_pku_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments%2FDGazeDataset&ga=1) | N/A                                                           | 2020 | Hu, Zhiming, et al. "Dgaze: Cnn-based gaze prediction in dynamic scenes." IEEE transactions on visualization and computer graphics 26.5 (2020): 1902-1911.<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 57                  | [https://zhiminghu.net/hu20_dgaze/pdf/hu20_dgaze.pdf](https://zhiminghu.net/hu20_dgaze/pdf/hu20_dgaze.pdf)                                                                                                                                                                               |
| Art Paintings                                                                    | Αn eye tracking experiment on 150 paintings belonging to 5 art movements, namely Fauvism, Impressionism, Pointillism, Realism and Romanticism                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Paintings                                                                          | Model visual attention                                                                                          | Remote/screen based | SMI RED eye-tracker                               | 60                                                                                                   | 150 paintings from Romantism, Realism, Impressionism, Pointillism and Fauvism                                                                                                             | [https://www-percept.irisa.fr/art_paintings/](https://www-percept.irisa.fr/art_paintings/)                                                                                                                             | Visual attention                              | Understand visual attention on art paintings                                                                                 | images, fixations                                                                                        | [https://www-percept.irisa.fr/art_paintings/](https://www-percept.irisa.fr/art_paintings/)                                                                                                                                                                                                                                                           | N/A                                                           | 2020 | Le Meur, Olivier, Tugdual Le Pen, and Rémi Cozot. "Can we accurately predict where we look at paintings?." Plos one 15.10 (2020): e0239980.<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | 7                   | [https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0239980](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0239980)                                                                                                                                   |
| EyeTrackUAV2                                                                     | Available data: raw gaze data, fixations, saccades, and heatmaps. Fixation detection process was based on the implementation of the I-DT based algorithm Scenario: all the above is available for both eyes, dominant eye, binocular position, left, and right eye.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | Survelance videos                                                                  | Model visual attention                                                                                          | Remote/screen based | EyeLink 1000 Plus                                 | 1000                                                                                                 | 30 observers                                                                                                                                                                              | [https://www-percept.irisa.fr/uav-datasets-eyetrackuav2/](https://www-percept.irisa.fr/uav-datasets-eyetrackuav2/)                                                                                                     | Visual attention                              | Saliency prediction, object detection, tracking, and recognition under both free viewing and task related viewing conditions | videos, fixations                                                                                        | [https://www-percept.irisa.fr/uav-datasets-eyetrackuav2/](https://www-percept.irisa.fr/uav-datasets-eyetrackuav2/)                                                                                                                                                                                                                                   | N/A                                                           | 2020 | Perrin, Anne-Flore, et al. "Eyetrackuav2: a large-scale binocular eye-tracking dataset for uav videos." Drones 4.1 (2020): 2.<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 16                  | [https://www.mdpi.com/2504-446X/4/1/2/pdf](https://www.mdpi.com/2504-446X/4/1/2/pdf)                                                                                                                                                                                                     |
| ETFP                                                                             | The primary purposes behind the two datasets are to study and analyse, respectively, colourblindness and object-attention.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Viewers were shown images, each one for 3 seconds, and their eye gaze was recorded | Analyse colourblindness and object-attention                                                                    | Remote/screen based | Tobii EyeX                                        | 55Hz                                                                                                 | 8 subjects                                                                                                                                                                                | [https://ieee-dataport.org/documents/etfp-eye-tracking-and-fixation-points#files](https://ieee-dataport.org/documents/etfp-eye-tracking-and-fixation-points#files)                                                     | Visual attention, Health                      | Understand color deficiency types and improve images for colour blind people                                                 | images, fixations                                                                                        | [https://ieee-dataport.org/documents/etfp-eye-tracking-and-fixation-points#files](https://ieee-dataport.org/documents/etfp-eye-tracking-and-fixation-points#files)                                                                                                                                                                                   | Creative Commons                                              | 2021 | Alessandro Bruno, Francesco Gugliuzza, March 19, 2021, "ETFP (Eye-Tracking and Fixation Points)", IEEE Dataport, doi: https://dx.doi.org/10.21227/0d1h-vb68.<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | N/A                 | [https://ieee-dataport.org/documents/etfp-eye-tracking-and-fixation-points#files](https://ieee-dataport.org/documents/etfp-eye-tracking-and-fixation-points#files)                                                                                                                       |
| EEGEyeNet                                                                        | We present a new dataset and benchmark with the goal of advancing<br>research in the intersection of brain activities and eye movements. Our<br>dataset, EEGEyeNet, consists of simultaneous Electroencephalography<br>(EEG) and Eye-tracking (ET) recordings from 356 different subjects collected from three different experimental paradigms. Using this dataset,<br>we also propose a benchmark to evaluate gaze prediction from EEG measurements. The benchmark consists of three tasks with an increasing level<br>of difficulty: left-right, angle-amplitude and absolute position. We run<br>extensive experiments on this benchmark in order to provide solid baselines, both based on classical machine learning models and on large neural<br>networks. We release our complete code and data and provide a simple<br>and easy-to-use interface to evaluate new methods.                                                                                                                                                                                                                                                                                                                                           | Synthesised dots and symbols in the screen                                         | Understand brain activities and eye gaze movements by predicting eye gaze from EEG                              | Remote/screen based | ET EyeLink 1000 Plus from SR Research             | 500 fps                                                                                              | 356 subjects                                                                                                                                                                              | [https://osf.io/vhuwf](https://osf.io/vhuwf)                                                                                                                                                                           | Neuroscience                                  | Advancing research in the intersection of brain activities and eye movements                                                 | 2D Screen Fixations, 2D Saliency, EEG                                                                    | [https://osf.io/ktv7m/](https://osf.io/ktv7m/)                                                                                                                                                                                                                                                                                                       | Creative Commons Attribution 4.0 International Public License | 2021 | Kastrati, Ard, et al. "EEGEyeNet: a simultaneous electroencephalography and eye-tracking dataset and benchmark for eye movement prediction." arXiv preprint arXiv:2111.05100 (2021).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 24                  | [https://arxiv.org/pdf/2111.05100.pdf](https://arxiv.org/pdf/2111.05100.pdf)                                                                                                                                                                                                             |
| GOO                                                                              | The GOO dataset is composed of images of shelves packed with 24 different classes of grocery items, where each image contains a human or a human mesh model gazing upon an object. All objects in the scene are annotated with their bounding box,class,and segmentation mask. As with existing gaze-related datasets, location and bounding box annotations for the person’s head are provided. With these annotations,GOO can also be used for other tasks such as object detection and segmentation. GOO Dataset consists of two parts: a larger synthetic set of images called GOO-Synth,and a smaller real set of images called GOO-Real.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | Natual images                                                                      | Model visual attention                                                                                          | N/A                 | N/A                                               | N/A                                                                                                  | 192,000 synthetic images and 9552 real images. The test set for the real data is made up of 2,156 images with the remaining images comprising the training set.                           | [https://github.com/upeee/GOO-GAZE2021/tree/main/dataset](https://github.com/upeee/GOO-GAZE2021/tree/main/dataset)                                                                                                     | Thrid person perspective for interactive gaze | Gaze target detection for retail use case                                                                                    | images, bounding boxes, segmentation maps, fixations                                                     | [https://github.com/upeee/GOO-GAZE2021](https://github.com/upeee/GOO-GAZE2021)                                                                                                                                                                                                                                                                       | Apache-2.0 license                                            | 2021 | Henri Tomas, Marcus Reyes,Raimarc Dionido,Mark Ty, Joel Casimiro,Rowel Atienza, and Richard Guinto. Goo: A dataset for gaze object prediction in retail environments. In CVPR Workshops(CVPRW),2021.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 30                  | [https://arxiv.org/pdf/2105.10793.pdf](https://arxiv.org/pdf/2105.10793.pdf)                                                                                                                                                                                                             |
| REFLACX                                                                          | The dataset contains 3,032 cases of synchronized eye-tracking and transcription pairs labeled by five radiologists.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | Medical images                                                                     | Model visual attention                                                                                          | Remote/screen based | Eyelink 1000 Plus                                 | 1000                                                                                                 | 5 radiologists and contains 3,032 synchronized sets of eye-tracking data and timestamped report transcriptions for 2,616 chest x-rays from the MIMIC-CXR dataset                          | [https://physionet.org/content/reflacx-xray-localization/1.0.0/](https://physionet.org/content/reflacx-xray-localization/1.0.0/)                                                                                       | Diagnosis prediction on Chest X-Rays          | Diagnosis prediction on Chest X-Rays                                                                                         | images, fixations                                                                                        | [https://physionet.org/content/reflacx-xray-localization/1.0.0/](https://physionet.org/content/reflacx-xray-localization/1.0.0/)                                                                                                                                                                                                                     | PhysioNet Restricted Health Data License 1.5.0                | 2021 | Bigolin Lanfredi, R., Zhang, M., Auffermann, W.F. et al. REFLACX, a dataset of reports and eye-tracking data for localization of abnormalities in chest x-rays. Sci Data 9, 350 (2022). https://doi.org/10.1038/s41597-022-01441-z<br><br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 19                  | [https://www.nature.com/articles/s41597-022-01441-z](https://www.nature.com/articles/s41597-022-01441-z)                                                                                                                                                                                 |
| Eye Gaze Data for Chest X-rays                                                   | The data was collected using an eye tracking system while a radiologist interpreted and read 1,083 public CXR images. The dataset contains the following aligned modalities: image, transcribed report text, dictation audio and eye gaze data.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | Medical images                                                                     | Model visual attention                                                                                          | Remote/screen based | Gazepoint. GP3 Eye Tracker.                       | N/A                                                                                                  | One radiologist reviewed and reported on 1,083 CXR image                                                                                                                                  | [https://physionet.org/content/egd-cxr/1.0.0/](https://physionet.org/content/egd-cxr/1.0.0/)                                                                                                                           | Diagnosis prediction on Chest X-Rays          | Diagnosis prediction on Chest X-Rays                                                                                         | images, fixations                                                                                        | [https://physionet.org/content/egd-cxr/1.0.0/](https://physionet.org/content/egd-cxr/1.0.0/)                                                                                                                                                                                                                                                         | PhysioNet Restricted Health Data License 1.5.0                | 2021 | Karargyris, A., Kashyap, S., Lourentzou, I. et al. Creation and validation of a chest X-ray dataset with eye-tracking and report dictation for AI development. Sci Data 8, 92 (2021). https://doi.org/10.1038/s41597-021-00863-5<br><br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | 69                  | [https://www.nature.com/articles/s41597-021-00863-5](https://www.nature.com/articles/s41597-021-00863-5)                                                                                                                                                                                 |
| Ego4D                                                                            | A massive-scale egocentric video dataset and benchmark suite containing 3,670 hours of daily-life activity video spanning hundreds of scenarios (household, outdoor, workplace, leisure, etc.) captured by 931 unique camera wearers from 74 worldwide locations and 9 different countries. The approach to collection is designed to uphold rigorous privacy and ethics standards, with consenting participants and robust de-identification procedures where relevant. Ego4D dramatically expands the volume of diverse egocentric video footage publicly available to the research community. Portions of the video are accompanied by audio, 3D meshes of the environment, eye gaze, stereo, and/or synchronized videos from multiple egocentric cameras at the same event. Furthermore, we present a host of new benchmark challenges centered around understanding the first-person visual experience in the past (querying an episodic memory), present (analyzing hand-object manipulation, audio-visual conversation, and social interactions), and future (forecasting activities). By publicly sharing this massive annotated dataset and benchmark suite, we aim to push the frontier of first-person perception. | Natural vision through glasses                                                     | Foundation dataset for egocentric computer vision                                                               | Near/glasses        | Pupil Labs,                                       | 200 Hz (Not entirely sure as it does not specify the model)                                          | 923 unique participants. Full dataset size 30+ TB, primary data for challenges 14 TB. Subset that includes gaze are 26 sessions with ~4 milion samples.                                   | [https://ego4d-data.org/docs/data/metadata/](https://ego4d-data.org/docs/data/metadata/)                                                                                                                               | Everyday activities                           | Computer Vision benchmark dataset for egocentric challenges                                                                  | RGB videos, annotations, IMU, raw gaze                                                                   | [https://ego4d-data.org/](https://ego4d-data.org/)                                                                                                                                                                                                                                                                                                   | EGO4D License                                                 | 2022 | Grauman K., Westbury A., Byrne E., Chavis Z., Furnari A., Girdhar R., Hamburger J., Jiang H., Liu M., Liu X., Martin M., Nagarajan T., Radosavovic I., Ramakrishnan S. K., Ryan F., Sharma J., Wray M., Xu M., Xu E. Z., Zhao C., Batra S. B. D., Cartillier V., Crane S., Do T., Doulaty M., Erapalli A., Feichtenhofer C., Fragomeni A., Fu Q., Gebreselasie A., González C., Hillis J., Huang X., Huang Y., Jia W., Khoo W., Kolář J., Kottur S., Kumar A., Landini F., Li C., Li Y., Li Z., Mangalam K., Modhugu R., Munro J., Murrell T., Nishiyasu T., Price W., Ruiz P., Ramazanova M., Sari L., Somasundaram K., Southerland A., Sugano Y., Tao R., Vo M., Wang Y., Wu X., Yagi T., Zhao Z., Zhu Y., Arbeláez P., Crandall D., Damen D., Farinella G. M., Fuegen C., Ghanem B., Ithapu V. K., Jawahar C. V., Joo H., Kitani K., Li H., Newcombe R., Oliva A., Park H. S., Rehg J. M., Sato Y., Shi J., Shou M. Z., Torralba A., Torresani L., Yan M., Malik J. Ego4D: Around the World in 3,000 Hours of Egocentric Video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022                                             | 437                 | [https://openaccess.thecvf.com/content/CVPR2022/papers/Grauman_Ego4D_Around_the_World_in_3000_Hours_of_Egocentric_Video_CVPR_2022_paper.pdf](https://openaccess.thecvf.com/content/CVPR2022/papers/Grauman_Ego4D_Around_the_World_in_3000_Hours_of_Egocentric_Video_CVPR_2022_paper.pdf) |
| eSEEd                                                                            | Eye movements of 48 participants were recorded as they watched 10 emotion evoking videos each of them followed by a neutral video                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | Movies                                                                             | Psychology analysis: estimation of an individual's affective state                                              | Mobile eye-tracker  | Pupil Labs                                        | N/A                                                                                                  | 48 participants (?)                                                                                                                                                                       | [https://zenodo.org/records/5913566](https://zenodo.org/records/5913566)                                                                                                                                               | Psychology                                    | Predict emotion from visual stimuli                                                                                          | videos, fixations                                                                                        | [https://zenodo.org/records/5913566](https://zenodo.org/records/5913566)                                                                                                                                                                                                                                                                             | Creative Commons Attribution 4.0 International<br>            | 2022 | V. Skaramagkas et al., "Review of Eye Tracking Metrics Involved in Emotional and Cognitive Processes," in IEEE Reviews in Biomedical Engineering, vol. 16, pp. 260-277, 2023, doi: 10.1109/RBME.2021.3066072. keywords: {Visualization;Measurement;Pupils;Cognitive processes;Task analysis;Physiology;Gaze tracking;Eye tracking;gaze;pupil;fixations;saccades;smooth pursuit;blinks;stress;visual attention;emotional arousal;cognitive workload;emotional arousal datasets;cognitive workload datasets},<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | 93                  | [https://ieeexplore.ieee.org/abstract/document/9380366](https://ieeexplore.ieee.org/abstract/document/9380366)                                                                                                                                                                           |
| CopCo: The Copenhagen Corpus of Eye-Tracking Recordings from Natural Reading<br> | It contains eye movement data from Danish native speakers, both from readers without dyslexia and readers with dyslexia. Additionally, there is a set of non-native speaking participants. The data contains one CSV file per participant with the computed eye-tracking metrics for each word                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | Sequence of text paragraphs displayed on screen.                                   | Understand reading processes                                                                                    | Remote/screen based | EyeLink 1000 Plus                                 | 1000                                                                                                 | 22 participants read Danish text                                                                                                                                                          | [https://osf.io/ud8s5/](https://osf.io/ud8s5/)                                                                                                                                                                         | Language understanding                        | Understanding natural language processing purposes between dyslexic and non-dyslexic individuals                             | text, fixations                                                                                          | [https://osf.io/ud8s5/](https://osf.io/ud8s5/)                                                                                                                                                                                                                                                                                                       | Creative Commons - Attribution 4.0 International (CC BY 4.0)  | 2022 | Hollenstein, Nora, Maria Barrett, and Marina Björnsdóttir. "The Copenhagen Corpus of eye tracking recordings from natural reading of Danish texts." arXiv preprint arXiv:2204.13311 (2022).<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | 8                   | [https://aclanthology.org/2022.lrec-1.182.pdf](https://aclanthology.org/2022.lrec-1.182.pdf)                                                                                                                                                                                             |
| Exo-Ego4D                                                                        | A diverse, large-scale multimodal multiview video dataset and benchmark challenge. Ego-Exo4D centers around simultaneously-captured egocentric and exocentric video of skilled human activities (e.g., sports, music, dance, bike repair). More than 800 participants from 13 cities worldwide performed these activities in 131 different natural scene contexts, yielding long-form captures from 1 to 42 minutes each and 1,422 hours of video combined. The multimodal nature of the dataset is unprecedented: the video is accompanied by multichannel audio, eye gaze, 3D point clouds, camera poses, IMU, and multiple paired language descriptions -- including a novel "expert commentary" done by coaches and teachers and tailored to the skilled-activity domain. To push the frontier of first-person video understanding of skilled human activity, we also present a suite of benchmark tasks and their annotations, including fine-grained activity understanding, proficiency estimation, cross-view translation, and 3D hand/body pose. All resources will be open sourced to fuel new research in the community.                                                                                           | Natural vision through glasses                                                     | Foundation dataset for egocentric computer vision                                                               | Near/glasses        | Meta Project Aria                                 | 10 Hz                                                                                                | 1422 hours of 839 unique participants                                                                                                                                                     | [https://docs.ego-exo4d-data.org/data/](https://docs.ego-exo4d-data.org/data/)                                                                                                                                         | Everyday activities                           | Computer Vision benchmark dataset for ego and exo centric challenges                                                         | RGB videos, annotations, IMU, raw gaze                                                                   | [https://ego-exo4d-data.org/](https://ego-exo4d-data.org/)                                                                                                                                                                                                                                                                                           | EXO-EGO4D License                                             | 2023 | Grauman K., Westbury A., Torresani L., Kitani K., Malik J., Afouras T., Ashutosh K., Baiyya V., Bansal S., Boote B., Byrne E., Chavis Z., Chen J., Cheng F., Chu F., Crane S., Dasgupta A., Dong J., Escobar M., Forigua C., Gebreselasie A., Haresh S., Huang J., Islam M. M., Jain S., Khirodkar R., Kukreja D., Liang K. J., Liu J., Majumder S., Mao Y., Martin M., Mavroudi E., Nagarajan T., Ragusa F., Ramakrishnan S., Seminara L., Somayazulu A., Song Y., Su S., Xue Z., Zhang E., Zhang J., Castillo A., Chen C., Fu X., Furuta R., González C., Gupta P., Hu J., Huang Y., Huang Y., Khoo W., Kumar A., Kuo R., Lakhavani S., Liu M., Luo M., Luo Z., Meredith B., Miller A., Oguntola O., Pan X., Peng P., Pramanick S., Ramazanova M., Ryan F., Shan W., Somasundaram K., Song C., Southerland A., Tateno M., Wang H., Wang Y., Yagi T., Yan M., Yang X., Yu Z., Zha S. C. , Zhao C., Zhao Z., Zhu Z., Zhuo J., Arbeláez P., Bertasius G., Crandall D., Damen D., Engel J., Farinella G. M., Furnari A., Ghanem B., Hoffman J., Jawahar C. V., Newcombe R., Park H. S., Rehg J. M., Sato Y., Savva Y., Shi J., Zheng M. Shou, and Michael Wray. ArXiv. 2023 | 3                   | [https://arxiv.org/abs/2311.18259](https://arxiv.org/abs/2311.18259)                                                                                                                                                                                                                     |
| CartoGAZE                                                                        | A large eye movement dataset produced during the observation of cartographic products.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Google maps                                                                        | Model visual attention                                                                                          | Remote/screen based | SMI RED250                                        | 250                                                                                                  | 38 participants                                                                                                                                                                           | [https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/ONIAZI](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/ONIAZI)                                                     | Visual attention                              | Understand visual attention of humans (experts vs novices) and                                                               | images, fixations                                                                                        | [https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/ONIAZI](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/ONIAZI)                                                                                                                                                                                   | CC0 1.0 DEED                                                  | 2023 | Keskin, M., Krassanakis, V., & Coltekin, A. (2023). Visual Attention and Recognition Differences Based on Expertise in a Map Reading and Memorability Study, In (eds. Popelka, S., Stachoň, Z., Kiefer, P., Çöltekin, A.) Eye-Tracking in Cartography Special Issue, IJGI - MDPI. doi: 10.3390/ijgi12010021 doi: 10.3390/ijgi12010021<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 4                   | [https://www.mdpi.com/2220-9964/12/1/21](https://www.mdpi.com/2220-9964/12/1/21)                                                                                                                                                                                                         |
| EyeLink 1000                                                                     | Eye movements of adults reading aloud short (four digit) and long (eight to 11 digit) Arabic numerals compared to matched-in-length words and pseudowords                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Arabic numbers                                                                     | Understand reading processes                                                                                    | Remote/screen based | EyeLink 1000                                      | 1000                                                                                                 | 36 students in psychology from the University of Lausanne (27 women and nine men; mean age = 21.3 years old; SD = 4.15).                                                                  | [https://zenodo.org/records/7962917](https://zenodo.org/records/7962917)                                                                                                                                               | Language understanding                        | Understand arabic number comprehension                                                                                       | text, fixations                                                                                          | [https://zenodo.org/records/7962917](https://zenodo.org/records/7962917)                                                                                                                                                                                                                                                                             | Creative Commons Attribution 4.0 International                | 2023 | de Chambrier, Anne-Françoise, et al. ""Reading numbers is harder than reading words: An eye-tracking study."" Acta Psychologica 237 (2023): 103942.<br>"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | 4                   | [https://doi.org/10.1016/j.actpsy.2023.103942](https://doi.org/10.1016/j.actpsy.2023.103942)                                                                                                                                                                                             |
