---
layout: home
search_exclude: true
image: /images/eye-gaze-large.png
toc: true
navigation_weight: 1
---
<p align="center" width="100%">
    <img width="20%" src="https://gaze-meets-ml.github.io/gaze_ml_2023/images/eye-gaze-large.png"> 
</p>

We are excited to host Gaze Meets ML workshop on **December, 2023** in conjunction with NeurIPS 2023. *The workshop will take place in-person at New Orleans!*

We’ve got a great lineup of [speakers](https://gaze-meets-ml.github.io/gaze_ml_2023/speakers/). We would like to thank our [sponsors](https://gaze-meets-ml.github.io/gaze_ml_2023/sponsors/) for their support. If you are interested in sponsoring, please find more information [here](https://gaze-meets-ml.github.io/gaze_ml_2023/call_for_sponsors/).

For questions and further information, please reach out to [gaze.neurips@gmail.com](mailto:gaze.neurips@gmail.com).

<img src="images/Logo_of_Twitter.svg.png" alt="Follow us on Twitter @Gaze_Meets_ML" width="18"> <a href="https://twitter.com/Gaze_Meets_ML">@Gaze_Meets_ML</a>



# About

Eye gaze has proven to be a cost-efficient way to collect large-scale physiological data that can reveal the underlying
human attentional patterns in real life workflows, and thus has long been explored as a signal to directly measure
human-related cognition in various domains. Physiological data (including but not limited to eye gaze) offer new
perception capabilities, which could be used in several ML domains, e.g., egocentric perception, embodiedAI, NLP, etc.
They can help infer human perception, intentions, beliefs, goals and other cognition properties that are much needed for
human-AI interactions and agent coordination. In addition, large collections of eye-tracking data have enabled
data-driven modeling of human visual attention mechanisms, both for saliency or scanpath prediction, with twofold
advantages: from the neuroscientific perspective to understand biological mechanisms better, from the AI perspective to
equip agents with the ability to mimic or predict human behavior and improve interpretability and interactions.

With the emergence of immersive technologies, now more than any time there is a need for experts of various backgrounds
(e.g., machine learning, vision, and neuroscience communities) to share expertise and contribute to a deeper
understanding of the intricacies of cost-efficient human supervision signals (e.g., eye-gaze) and their utilization
towards by bridging human cognition and AI in machine learning research and development. The goal of this workshop is to
bring together an active research community to collectively drive progress in defining and addressing core problems in
gaze-assisted machine learning.

For past <span style="color:MediumSeaGreen">Gaze Meets ML</span> workshop, visit <a href="https://gaze-meets-ml.github.io/gaze_ml_2022/">Gaze Meets ML 2022.</a>

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-bd63{color:#212529;font-style:italic;text-align:left;vertical-align:top}
.tg .tg-1rwr{color:#008000;text-align:left;vertical-align:top}
.tg .tg-av16{color:#212529;text-align:left;vertical-align:top}
.tg .tg-ndde{color:#2294E0;font-weight:bold;text-align:left;vertical-align:top}
.tg .tg-w1dh{color:#212529;font-weight:bold;text-align:left;vertical-align:top}
.tg .tg-0lax{text-align:left;vertical-align:top}
</style>




<!-- <table class="tg">
<thead>
  <tr>
    <th class="tg-av16">Sat 7:30 a.m. - 8:00 a.m.</th>
    <th class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse63683" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">Meet and Greet and Getting started</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse63683" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Break)</span></th>
    <th class="tg-bd63"></th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-1rwr"><br>Sat 8:00 a.m. - 8:10 a.m.</td>
    <td class="tg-w1dh"><span style="font-weight:bolder">Opening Remarks (10 mins) Organizers</span> <span style="font-weight:400">(Opening Remarks)</span></td>
    <td class="tg-bd63"></td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 8:10 a.m. - 8:55 a.m.</td>
    <td class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse63682" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">Learning gaze control, external attention, and internal attention since 1990-91</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse63682" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Keynote)</span></td>
    <td class="tg-bd63">Jürgen Schmidhuber</td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 9:00 a.m. - 9:30 a.m.</td>
    <td class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse63680" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">Eye-tracking what's going on in the mind</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse63680" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Keynote)</span></td>
    <td class="tg-bd63">Tobias Gerstenberg</td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 9:30 a.m. - 10:00 a.m.</td>
    <td class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse63807" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">Neural encoding and decoding of facial movements</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse63807" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Keynote)</span></td>
    <td class="tg-bd63">Scott Linderman</td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 10:00 a.m. - 10:15 a.m.</td>
    <td class="tg-w1dh"><span style="font-weight:bolder">Coffee Break</span> <span style="font-weight:400">(Break)</span></td>
    <td class="tg-bd63"></td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 10:20 a.m. - 10:32 a.m.</td>
    <td class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57746" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">Electrode Clustering and Bandpass Analysis of EEG Data for Gaze Estimation</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57746" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Spotlight)</span><br></td>
    <td class="tg-bd63">Ard Kastrati · Martyna Plomecka · Joël Küchler · Nicolas Langer · Roger Wattenhofer</td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 10:32 a.m. - 10:44 a.m.</td>
    <td class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57751" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">Modeling Human Eye Movements with Neural Networks in a Maze-Solving Task</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57751" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Spotlight)</span></td>
    <td class="tg-bd63">Jason Li · Nicholas Watters · Sandy Wang · Hansem Sohn · Mehrdad Jazayeri</td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 10:44 a.m. - 10:56 a.m.</td>
    <td class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57755" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">Intention Estimation via Gaze for Robot Guidance in Hierarchical Tasks</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57755" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Spotlight)</span></td>
    <td class="tg-bd63">Yifan Shen · Xiaoyu Mo · Vytas Krisciunas · David Hanson · Bertram Shi</td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 10:56 a.m. - 11:08 a.m.</td>
    <td class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57759" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">Facial Composite Generation with Iterative Human Feedback</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57759" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Spotlight)</span></td>
    <td class="tg-bd63">Florian Strohm · Ekta Sood · Dominike Thomas · Mihai Bace · Andreas Bulling</td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 11:08 a.m. - 11:20 a.m.</td>
    <td class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57762" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">Simulating Human Gaze with Neural Visual Attention</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57762" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Spotlight)</span></td>
    <td class="tg-bd63">Leo Schwinn · Doina Precup · Bjoern Eskofier · Dario Zanca</td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 11:20 a.m. - 11:50 a.m.</td>
    <td class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse63810" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">Foveated Models of Visual Search and Medical Image Perception</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse63810" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Keynote)</span></td>
    <td class="tg-bd63">Miguel Eckstein</td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 12:00 p.m. - 12:30 p.m.</td>
    <td class="tg-w1dh"><span style="font-weight:bolder">Lunch</span> <span style="font-weight:400">(Lunch and Poster Walk-Around)</span></td>
    <td class="tg-bd63"></td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 12:30 p.m. - 1:30 p.m.</td>
    <td class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57742" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">Appearance-Based Gaze Estimation for Driver Monitoring</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57742" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Poster)</span></td>
    <td class="tg-bd63">Soodeh Nikan · Devesh Upadhyay</td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 12:30 p.m. - 1:30 p.m.</td>
    <td class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57743" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">Selection of XAI Methods Matters: Evaluation of Feature Attribution Methods for Oculomotoric Biometric Identification</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57743" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Poster)</span></td>
    <td class="tg-bd63">Daniel Krakowczyk · David Robert Reich · Paul Prasse · Sebastian Lapuschkin · Lena A. Jäger · Tobias Scheffer</td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 12:30 p.m. - 1:30 p.m.</td>
    <td class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57744" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">Time-to-Saccade metrics for real-world evaluation</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57744" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Poster)</span></td>
    <td class="tg-bd63">Tim Rolff · Niklas Stein · Markus Lappe · Frank Steinicke · Simone Frintrop</td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 12:30 p.m. - 1:30 p.m.</td>
    <td class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57745" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">Electrode Clustering and Bandpass Analysis of EEG Data for Gaze Estimation</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57745" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Poster)</span></td>
    <td class="tg-bd63">Ard Kastrati · Martyna Plomecka · Joël Küchler · Nicolas Langer · Roger Wattenhofer</td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 12:30 p.m. - 1:30 p.m.</td>
    <td class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57747" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">Skill, or Style? Classification of Fetal Sonography Eye-Tracking Data</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57747" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Poster)</span></td>
    <td class="tg-bd63">Clare Teng · Lior Drukker · Aris Papageorghiou · Alison Noble</td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 12:30 p.m. - 1:30 p.m.</td>
    <td class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57748" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">Decoding Attention from Gaze: A Benchmark Dataset and End-to-End Models</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57748" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Poster)</span></td>
    <td class="tg-bd63">Karan Uppal · Jaeah Kim · Shashank Singh</td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 12:30 p.m. - 1:30 p.m.</td>
    <td class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57749" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">Learning to count visual objects by combining "what" and "where" in recurrent memory</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57749" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Poster)</span></td>
    <td class="tg-bd63">Jessica Thompson · Hannah Sheahan · Christopher Summerfield</td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 12:30 p.m. - 1:30 p.m.</td>
    <td class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57750" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">Modeling Human Eye Movements with Neural Networks in a Maze-Solving Task</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57750" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Poster)</span><br></td>
    <td class="tg-bd63">Jason Li · Nicholas Watters · Sandy Wang · Hansem Sohn · Mehrdad Jazayeri</td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 12:30 p.m. - 1:30 p.m.</td>
    <td class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57752" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">Generating Attention Maps from Eye-gaze for the Diagnosis of Alzheimer's Disease</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57752" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Poster)</span></td>
    <td class="tg-bd63">Carlos Antunes · Margarida Silveira</td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 12:30 p.m. - 1:30 p.m.</td>
    <td class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57753" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">Do They Look Where They Go? Gaze Classification During Walking</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57753" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Poster)</span></td>
    <td class="tg-bd63">Gianni Bremer · Niklas Stein · Markus Lappe</td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 12:30 p.m. - 1:30 p.m.</td>
    <td class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57754" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">Intention Estimation via Gaze for Robot Guidance in Hierarchical Tasks</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57754" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Poster)</span></td>
    <td class="tg-bd63">Yifan Shen · Xiaoyu Mo · Vytas Krisciunas · David Hanson · Bertram Shi</td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 12:30 p.m. - 1:30 p.m.</td>
    <td class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57756" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">Comparing radiologists' gaze and saliency maps generated by interpretability methods for chest x-rays</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57756" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Poster)</span></td>
    <td class="tg-bd63">Ricardo Bigolin Lanfredi · Ambuj Arora · Trafton Drew · Joyce Schroeder · Tolga Tasdizen</td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 12:30 p.m. - 1:30 p.m.</td>
    <td class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57757" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">Integrating eye gaze into machine learning using fractal curves</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57757" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Poster)</span></td>
    <td class="tg-bd63">Robert Ahadizad Newport · Sidong Liu · Antonio Di Ieva</td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 12:30 p.m. - 1:30 p.m.</td>
    <td class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57758" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">Facial Composite Generation with Iterative Human Feedback</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57758" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Poster)</span></td>
    <td class="tg-bd63">Florian Strohm · Ekta Sood · Dominike Thomas · Mihai Bace · Andreas Bulling</td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 12:30 p.m. - 1:30 p.m.</td>
    <td class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57760" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">Federated Learning for Appearance-based Gaze Estimation in the Wild</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57760" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Poster)</span></td>
    <td class="tg-bd63">Mayar Elfares · Zhiming Hu · Pascal Reisert · Andreas Bulling · Ralf Küsters</td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 12:30 p.m. - 1:30 p.m.</td>
    <td class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57761" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">Simulating Human Gaze with Neural Visual Attention</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57761" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Poster)</span></td>
    <td class="tg-bd63">Leo Schwinn · Doina Precup · Bjoern Eskofier · Dario Zanca</td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 12:30 p.m. - 1:30 p.m.</td>
    <td class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57763" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">Contrastive Representation Learning for Gaze Estimation</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57763" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Poster)</span></td>
    <td class="tg-bd63">Swati Jindal · Roberto Manduchi</td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 12:30 p.m. - 1:30 p.m.</td>
    <td class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57765" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">SecNet: Semantic Eye Completion in Implicit Field</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57765" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Poster)</span></td>
    <td class="tg-bd63">Yida Wang · Yiru Shen · David Joseph Tan · Federico Tombari · Sachin S Talathi</td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 1:30 p.m. - 2:00 p.m.</td>
    <td class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse63809" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">Use of Machine Learning and Gaze Tracking to Predict Radiologists’ Decisions in Breast Cancer Detection</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse63809" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Keynote)</span></td>
    <td class="tg-bd63">Claudia Mello-Thoms</td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 2:00 p.m. - 2:30 p.m.</td>
    <td class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse63808" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">Gabriel A. Silva Keynote</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse63808" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Keynote)</span></td>
    <td class="tg-bd63">Gabriel Silva</td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 2:30 p.m. - 3:30 p.m.</td>
    <td class="tg-w1dh"><span style="font-weight:bolder">Breakout session</span> <span style="font-weight:400">(Discussion within onsite small groups on preselected themes)</span></td>
    <td class="tg-bd63"></td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 3:30 p.m. - 3:45 p.m.</td>
    <td class="tg-w1dh"><span style="font-weight:bolder">Coffee</span> <span style="font-weight:400">(Break)</span></td>
    <td class="tg-bd63"></td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 3:45 p.m. - 3:57 p.m.</td>
    <td class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57764" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">Contrastive Representation Learning for Gaze Estimation</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57764" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Spotlight)</span></td>
    <td class="tg-bd63">Swati Jindal · Roberto Manduchi</td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 3:57 p.m. - 4:09 p.m.</td>
    <td class="tg-ndde"><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57766" target="_blank" rel="noopener noreferrer"><span style="font-weight:bolder">SecNet: Semantic Eye Completion in Implicit Field</span></a><a href="https://neurips.cc/virtual/2023/workshop/49990#collapse57766" target="_blank" rel="noopener noreferrer"> </a><span style="font-weight:400">(Spotlight)</span></td>
    <td class="tg-bd63">Yida Wang · Yiru Shen · David Joseph Tan · Federico Tombari · Sachin S. Talathi</td>
  </tr>
  <tr>
    <td class="tg-1rwr"><br>Sat 4:45 p.m. - 5:00 p.m.</td>
    <td class="tg-w1dh"><span style="font-weight:bolder">Wrap Up - Closing remarks</span> <span style="font-weight:400">(Closing)</span></td>
    <td class="tg-0lax"></td>
  </tr>
</tbody>
</table> -->

# Tentative Program
<ul>
    <li>Morning session (225 mins):
        <ul>
            <li>Opening Remarks (15 minutes)</li>
            <li>Keynote (45 mins)</li>
            <li>Invited talks (3x30min = 90 mins) </li>
            <li>Break (15 mins)</li>
            <li>Papers Presentation (60 mins)</li>
        </ul>
    </li>
    <li>Lunch (60 mins):
        <ul>
            <li>60 mins with a walk around poster session</li>
        </ul>
    </li>
    <li> Afternoon session (175 mins):
        <ul>
            <li>Invited talks (1x30 = 30 mins) </li>
            <li>Coffee Break (15 mins)</li>
            <li>Breakout Session (90 mins)</li>
            <li>Papers Presentation (60 mins) </li>
            <li>Closing Remarks (10 mins)</li>
        </ul>
    </li>
</ul>

All times are in Central Time

# Call for Papers

We welcome submissions that present aspects of eye-gaze in regards to cognitive science, psychophysiology and computer
science, or propose methods on integrating eye gaze into machine learning. We are also looking for applications from radiology, AR/VR,
autonomous driving, etc. that introduce methods and models utilizing eye gaze technology in their respective domains.

Topics of interest include but are not limited to the following:

<ul>
    <li>Understanding the neuroscience of eye-gaze and perception.</li>
    <li>State of the art in incorporating machine learning and eye-tracking.</li>
    <li>Annotation and ML supervision with eye-gaze.</li>
    <li>Attention mechanisms and their correlation with eye-gaze.</li>
    <li>Methods for gaze estimation and prediction using machine learning.</li>
    <li>Unsupervised ML using eye gaze information for feature importance/selection.</li>
    <li>Understanding human intention and goal inference.</li>
    <li>Using saccadic vision for ML applications.</li>
    <li>Use of gaze for human-AI interaction and agent coordination in multi-agent environments.</li>
    <li>Eye gaze used for AI, e.g., NLP, Computer Vision, RL, Explainable AI, Embodied AI, Trustworthy AI.</li>
    <li>Ethics of Eye Gaze in AI</li>
    <li>Gaze applications in cognitive psychology, radiology, neuroscience, AR/VR, autonomous cars, privacy, etc.</li>
</ul>

# Important Dates

<ul>Submission due: <b>27th September 2023</b></ul>
<ul>Reviewing starts: <b>30th September 2023</b></ul>
<ul>Reviewing ends: <b>16th October 2023</b></ul>
<ul>Notification of acceptance: <b>27th October 2023</b></ul>
<ul>SlideLive presentation pre-recording upload for NeurIPS (hard deadline): <b>10th November 2023</b></ul>
<ul>Camera ready paper: <b>17th November 2023</b></ul>
<ul>Workshop Date: <b>15th or 16th December 2023</b></ul>

# Submissions

The workshop will feature two tracks for submission: a **full, archival proceedings track** with accepted papers published in the [Proceedings for Machine Learning Research (PMLR)](https://proceedings.mlr.press/); and a **non-archival, extended abstract track**. Submissions to either track will undergo the same double-blind peer review. Full proceedings papers can be up to 15 pages and extended abstract papers can be up to 8 pages (both excluding references and appendices). Authors of accepted extended abstracts (non-archival submissions) retain full copyright of their work, and acceptance of such a submission to Gaze Meets ML does not preclude publication of the same material in another archival venue (e.g., journal or conference).

<ul>
    <li><a href="https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/Gaze_Meets_ML">Open Review Submission Portal</a></li>
    <li> Submission templates: <a href="https://ctan.org/tex-archive/macros/latex/contrib/jmlr">PMLR LaTeX template</a></li>
    <li>References and appendix should be appended into the same (single) PDF document, and do not count towards the page count.</li>
</ul>


# FAQs

For a list of commonly asked questions, please see <a href="https://gaze-meets-ml.github.io/gaze_ml_2023/faq/">FAQs</a>

<!-- # Keynote Speaker
[<img src="https://gaze-meets-ml.github.io/gaze_ml_2023/images/Juergen-Schmidhuber_cropped.jpg" width="200"/>](speakers#schmidhuber)

[**Jürgen Schmidhuber, Ph.D.**](speakers#schmidhuber) -->

<!-- Abstact: First I’ll discuss our early work of 1990-91 on learning internal attention and on attentive neural networks that learn to steer foveas; then I’ll mention what happened in the subsequent 3 decades. In preparation of this workshop, I made two overview web sites: 1. End-to-End Differentiable Sequential Neural Attention 1990-93 https://people.idsia.ch/~juergen/neural-attention-1990-1993.html. 2. Learning internal spotlights of attention with what’s now called "Transformers with linearized self-attention" which are formally equivalent to my 1991 Fast Weight Programmers: https://people.idsia.ch/~juergen/fast-weight-programmer-1991-transformer.html -->

<!-- # Speakers <a name="invited_speakers"></a>

| [<img src="https://gaze-meets-ml.github.io/gaze_ml_2023/images/Scott_Linderman_cropped.jpg" width="200"/>](speakers#linderman) | [<img src="https://gaze-meets-ml.github.io/gaze_ml_2023/images/Gabriel_Silva_cropped.jpg" width="200"/>](speakers#silva) | [<img src="https://gaze-meets-ml.github.io/gaze_ml_2023/images/Claudia-Mello-Thoms_cropped.jpg" width="200"/>](speakers#mellothoms) |
| :-: | :-: | :-: |
| [**Scott W. Linderman, Ph.D.**](speakers#linderman) | [**Gabriel A. Silva, Ph.D.**](speakers#silva) | [**Claudia Mello-Thoms, MS, Ph.D.**](speakers#mellothoms) |
| **Stanford** | **UC San Diego** | **University of Iowa** |

| [<img src="https://gaze-meets-ml.github.io/gaze_ml_2023/images/Eckstein_Miguel_cropped.jpg" width="200"/>](speakers#eckstein) | [<img src="https://gaze-meets-ml.github.io/gaze_ml_2023/images/Tobias_Gerstenberg_cropped.jpg" width="200"/>](speakers#gerstenberg) |
| :-: | :-: |
| [**Miguel P. Eckstein, Ph.D.**](speakers#eckstein) | [**Tobias Gerstenberg, MSc, Ph.D.**](speakers#gerstenberg) |
| **UC Santa Barbara** | **Stanford** | -->

# Organizers <a name="organizers"></a>
<table>
  <thead>
    <tr>
      <th style="text-align: center;width: 33.33%;"><a href="https://sites.google.com/vt.edu/amarachiblessingmbakwe/home"><img src="https://gaze-meets-ml.github.io/gaze_ml_2023/images/Amarachi_Mbakwe_cropped.png" width="200"></a></th>
      <th style="text-align: center;width: 33.33%;"><a href="https://scholar.google.com/citations?user=03O8mIMAAAAJ&amp;hl=en"><img src="https://gaze-meets-ml.github.io/gaze_ml_2023/images/Joy_Wu_cropped.png" width="200"></a></th>
      <th style="text-align: center;width: 33.33%;"><a href="https://www.mad.tf.fau.de/person/dario-zanca/"><img src="https://gaze-meets-ml.github.io/gaze_ml_2023/images/Dario_Zanca_cropped.png" width="200"></a></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="https://sites.google.com/vt.edu/amarachiblessingmbakwe/home"><strong>Amarachi Mbakwe, MS</strong></a></td>
      <td style="text-align: center"><a href="https://scholar.google.com/citations?user=03O8mIMAAAAJ&amp;hl=en"><strong>Joy Tzung-yu Wu, MD, MPH.</strong></a></td>
      <td style="text-align: center"><a href="https://www.mad.tf.fau.de/person/dario-zanca/"><strong>Dario Zanca, Ph.D.</strong></a></td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Virginia Tech</strong></td>
      <td style="text-align: center"><strong>Stanford, IBM Research</strong></td>
      <td style="text-align: center"><strong>Friedrich-Alexander-Universität Erlangen-Nürnberg</strong></td>
    </tr>
  </tbody>
</table>
<table>
  <thead>
    <tr>
      <th style="text-align: center;width: 33.33%;"><a href="https://scholar.google.com/citations?user=IFy7aPsAAAAJ&amp;hl=en"><img src="https://gaze-meets-ml.github.io/gaze_ml_2023/images/Elizabeth_Krupinski_cropped.png" width="190"></a></th>
      <th style="text-align: center;width: 33.33%;"><a href="https://researcher.watson.ibm.com/researcher/view.php?person=ibm-Satyananda.Kashyap"><img src="https://gaze-meets-ml.github.io/gaze_ml_2023/images/Satyananda_Kashyap_cropped.png" width="200"></a></th>
      <th style="text-align: center;width: 33.33%;"><a href="https://www.linkedin.com/in/alexandroskarargyris/"><img src="https://gaze-meets-ml.github.io/gaze_ml_2023/images/Alexandros_Karargyris_cropped.png" width="200"></a></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="https://scholar.google.com/citations?user=IFy7aPsAAAAJ&amp;hl=en"><strong>Elizabeth A. Krupinski, PhD FSPIE, FSIIM, FATA, FAIMBE</strong></a></td>
      <td style="text-align: center"><a href="https://researcher.watson.ibm.com/researcher/view.php?person=ibm-Satyananda.Kashyap"><strong>Satyananda Kashyap, Ph.D.</strong></a></td>
      <td style="text-align: center"><a href="https://www.linkedin.com/in/alexandroskarargyris/"><strong>Alexandros Karargyris, Ph.D.</strong></a></td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Emory University</strong></td>
      <td style="text-align: center"><strong>IBM Research</strong></td>
      <td style="text-align: center"><strong>MLCommons</strong></td>
    </tr>
  </tbody>
</table>

# Program Committee <a name="program_committee"></a>

<ul>
<li> Anna Lisa Gentile (IBM Research) </li>
<li> Brendan David-John (Virginia Tech) </li>
<li> Daniel Gruhl (IBM Research) </li>
<li> Efe Bozkir (University of Tuebingen) </li>
<li> Ehsan Degan (IBM Research) </li>
<li> G Anthony Reina (Intel) </li>
<li> Henning Müller (HES-SO Valais) </li>
<li> Hoda Eldardiry (Virginia Tech) </li>
<li> Hongzhi Wang (IBM Research) </li>
<li> Junwen Wu (Intel) </li>
<li> Kamran Binaee (RIT) </li>
<li> Ken C. L. Wong (IBM Research) </li>
<li> Maria Xenochristou (Stanford University) </li>
<li> Megan T deBettencourt (University of Chicago) </li>
<li> Mehdi Moradi (Google) </li>
<li> Neerav Karani (MIT) </li>
<li> Niharika Shimona D'Souza (IBM Research) </li>
<li> Nishant Rai (Stanford University) </li>
<li> Peter Mattson (Google) </li>
<li> Prashant Shah (Intel) </li>
<li> Safa Messaoud (Qatar Computing Research Institute) </li>
<li> Sameer Antani (NIH) </li>
<li> Sayan Ghosal (Johns Hopkins University) </li>
<li> Shiye Cao (Johns Hopkins University) </li>
<li> Sivarama Krishnan Rajaraman (NIH) </li>
<li> Spyridon Bakas (University of Pennsylvania) </li>
<li> Szilard Vajda (Central Washington University) </li>
<li> Vy A. Vo (Intel) </li>
<li> Wolfgang Mehringer (Friedrich-Alexander-Universität Erlangen-Nürnberg) </li>
<li> Leo Schwinn (Friedrich-Alexander-Universität Erlangen-Nürnberg) </li>  
<li> Thomas Altstidl (Friedrich-Alexander-Universität Erlangen-Nürnberg) </li> 
<li> Kai Kohlhoff (Google Research) </li> 
<li> Matteo Tiezzi (University of Siena) </li>      
</ul>

<!-- # Endorsements & Acknowledgements
- We are a MICCAI endorsed event:
<p align="left" width="100%">
    <img width="40%" src="https://gaze-meets-ml.github.io/gaze_ml_2022/images/MICCAI_logo_sm.jpg">
</p>

- <p align="left" width="100%">
    Eye gaze logo designed by Michael Chung</p> -->
